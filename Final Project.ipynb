{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Emotion From EEG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    My project tries to use quantitative EEG data to predict emotion. The availability of data in the form needed for this analysis was scarce. However, I was able to find very well collected data by Jordan Bird a PhD researcher from Aston University in the UK.\n",
    "\n",
    "https://www.researchgate.net/publication/329403546_Mental_Emotional_Sentiment_Classification_with_an_EEG-based_Brain-machine_Interface (related research)\n",
    "\n",
    "    The dataset is of EEG brainwave data that has been processed in a way to be expressed qualitatively. (qEEG or quantitative EEG is above the scope of my understanding, an explanation of what it is can be found at this link https://qeegsupport.com/what-is-qeeg-or-brain-mapping/#:~:text=Quantitative%20EEG%20(qEEG)%20is%20the,the%20EEG%20and%20brain%20function.) The data was collected from two people (1 male and 1 female) for 3 minutes at three different states which were defined as negative, positive and neutral). The negative and positive states were induced by using video images from movies and other sources.\n",
    "    \n",
    "   I took this data and processed it to change the labels from negative, neutral and positive to -1,0,1. After doing so did a train test spil of 70% to 30% and trained it on a SVC, decision tree classifier, K nearest neighbor, Random Forest, and Multiple Layer Perceptronto see which model would work best. On all models checked the accuracy using 10-fold cross validation and checking the accuracy on the test split. The results are submitted below in a table. I also tried majority voting from SVC, decision tree, KNN and Random forest ensemble, From the different type of classifiers Random Forest, did the best on both test split accuracy and 10 fold cross validation accuracy. \n",
    "    The results look very promising, from the table below, it can be shown that it is possible to train a machine model to look and analyze brain data to get meaningful information. This has implication that over time we can get to a better level of understanding of the brain. This can be used in analyzing emotional conditions and maybe even tell if people are telling the truth. Also the ability to analyze EEG data can lead to communication directly from the brain that can be understood, which can help people that are paralyzed or in general being able to check the health of the brain. \n",
    "    Eventhough I was unable to get more qualitative EEG data from other sources, additional work can be done on this project running the model on data collected from other sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.620</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.700</td>\n",
       "      <td>2.060</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.50</td>\n",
       "      <td>20.300</td>\n",
       "      <td>20.300</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.800</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.80</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.880</td>\n",
       "      <td>3.830</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.900</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.70</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.200</td>\n",
       "      <td>89.900</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.900</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.820</td>\n",
       "      <td>2.300</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.00</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>299.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.060</td>\n",
       "      <td>41.400</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>38.100</td>\n",
       "      <td>38.100</td>\n",
       "      <td>12.00</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>32.400</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>30.80</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>95.2</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>47.20</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>16.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>-284.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>23.9</td>\n",
       "      <td>4.200</td>\n",
       "      <td>1.090</td>\n",
       "      <td>4.460</td>\n",
       "      <td>4.720</td>\n",
       "      <td>6.63</td>\n",
       "      <td>...</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-35.5</td>\n",
       "      <td>142.00</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>142.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>-0.547</td>\n",
       "      <td>28.3</td>\n",
       "      <td>-259.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>26.7</td>\n",
       "      <td>9.080</td>\n",
       "      <td>6.900</td>\n",
       "      <td>12.700</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>370.00</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>370.00</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>16.800</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-288.0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.580</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>1.690</td>\n",
       "      <td>4.74</td>\n",
       "      <td>...</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>552.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>552.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>27.000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>25.00</td>\n",
       "      <td>28.9</td>\n",
       "      <td>4.990</td>\n",
       "      <td>1.950</td>\n",
       "      <td>6.210</td>\n",
       "      <td>3.490</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.95</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  \\\n",
       "0          4.620      30.3    -356.0     15.60      26.3       1.070   \n",
       "1         28.800      33.1      32.0     25.80      22.8       6.550   \n",
       "2          8.900      29.4    -416.0     16.70      23.7      79.900   \n",
       "3         14.900      31.6    -143.0     19.80      24.3      -0.584   \n",
       "4         28.300      31.3      45.2     27.30      24.5      34.800   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "2127      32.400      32.2      32.2     30.80      23.4       1.640   \n",
       "2128      16.300      31.3    -284.0     14.30      23.9       4.200   \n",
       "2129      -0.547      28.3    -259.0     15.80      26.7       9.080   \n",
       "2130      16.800      19.9    -288.0      8.34      26.0       2.460   \n",
       "2131      27.000      32.0      31.8     25.00      28.9       4.990   \n",
       "\n",
       "      mean_d_1_a  mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  \\\n",
       "0          0.411     -15.700       2.060        3.15  ...      23.50   \n",
       "1          1.680       2.880       3.830       -4.82  ...     -23.30   \n",
       "2          3.360      90.200      89.900        2.03  ...     462.00   \n",
       "3         -0.284       8.820       2.300       -1.97  ...     299.00   \n",
       "4         -5.790       3.060      41.400        5.52  ...      12.00   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2127      -2.030       0.647      -0.121       -1.10  ...     -21.70   \n",
       "2128       1.090       4.460       4.720        6.63  ...     594.00   \n",
       "2129       6.900      12.700       2.030        4.64  ...     370.00   \n",
       "2130       1.580     -16.000       1.690        4.74  ...     124.00   \n",
       "2131       1.950       6.210       3.490       -3.51  ...       1.95   \n",
       "\n",
       "      fft_742_b  fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  \\\n",
       "0        20.300     20.300      23.50     -215.0     280.00    -162.00   \n",
       "1       -21.800    -21.800     -23.30      182.0       2.57     -31.60   \n",
       "2      -233.000   -233.000     462.00     -267.0     281.00    -148.00   \n",
       "3      -243.000   -243.000     299.00      132.0     -12.40       9.53   \n",
       "4        38.100     38.100      12.00      119.0     -17.60      23.90   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2127      0.218      0.218     -21.70       95.2     -19.90      47.20   \n",
       "2128   -324.000   -324.000     594.00      -35.5     142.00     -59.80   \n",
       "2129   -160.000   -160.000     370.00      408.0    -169.00     -10.50   \n",
       "2130    -27.600    -27.600     124.00     -656.0     552.00    -271.00   \n",
       "2131      1.810      1.810       1.95      110.0      -6.71      22.80   \n",
       "\n",
       "      fft_748_b  fft_749_b     label  \n",
       "0       -162.00     280.00  NEGATIVE  \n",
       "1        -31.60       2.57   NEUTRAL  \n",
       "2       -148.00     281.00  POSITIVE  \n",
       "3          9.53     -12.40  POSITIVE  \n",
       "4         23.90     -17.60   NEUTRAL  \n",
       "...         ...        ...       ...  \n",
       "2127      47.20     -19.90   NEUTRAL  \n",
       "2128     -59.80     142.00  POSITIVE  \n",
       "2129     -10.50    -169.00  NEGATIVE  \n",
       "2130    -271.00     552.00  NEGATIVE  \n",
       "2131      22.80      -6.71   NEUTRAL  \n",
       "\n",
       "[2132 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# import data file\n",
    "\n",
    "emotion = pd.read_csv('emotions.csv')\n",
    "\n",
    "# Print data set\n",
    "\n",
    "emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18c93ff34c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.620</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.700</td>\n",
       "      <td>2.060</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.50</td>\n",
       "      <td>20.300</td>\n",
       "      <td>20.300</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.800</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.80</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.880</td>\n",
       "      <td>3.830</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.900</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.70</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.200</td>\n",
       "      <td>89.900</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.900</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.820</td>\n",
       "      <td>2.300</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.00</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>299.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.060</td>\n",
       "      <td>41.400</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>38.100</td>\n",
       "      <td>38.100</td>\n",
       "      <td>12.00</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>32.400</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>30.80</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>95.2</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>47.20</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>16.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>-284.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>23.9</td>\n",
       "      <td>4.200</td>\n",
       "      <td>1.090</td>\n",
       "      <td>4.460</td>\n",
       "      <td>4.720</td>\n",
       "      <td>6.63</td>\n",
       "      <td>...</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-35.5</td>\n",
       "      <td>142.00</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>142.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>-0.547</td>\n",
       "      <td>28.3</td>\n",
       "      <td>-259.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>26.7</td>\n",
       "      <td>9.080</td>\n",
       "      <td>6.900</td>\n",
       "      <td>12.700</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>370.00</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>370.00</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>16.800</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-288.0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.580</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>1.690</td>\n",
       "      <td>4.74</td>\n",
       "      <td>...</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>552.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>552.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>27.000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>25.00</td>\n",
       "      <td>28.9</td>\n",
       "      <td>4.990</td>\n",
       "      <td>1.950</td>\n",
       "      <td>6.210</td>\n",
       "      <td>3.490</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.95</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  \\\n",
       "0          4.620      30.3    -356.0     15.60      26.3       1.070   \n",
       "1         28.800      33.1      32.0     25.80      22.8       6.550   \n",
       "2          8.900      29.4    -416.0     16.70      23.7      79.900   \n",
       "3         14.900      31.6    -143.0     19.80      24.3      -0.584   \n",
       "4         28.300      31.3      45.2     27.30      24.5      34.800   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "2127      32.400      32.2      32.2     30.80      23.4       1.640   \n",
       "2128      16.300      31.3    -284.0     14.30      23.9       4.200   \n",
       "2129      -0.547      28.3    -259.0     15.80      26.7       9.080   \n",
       "2130      16.800      19.9    -288.0      8.34      26.0       2.460   \n",
       "2131      27.000      32.0      31.8     25.00      28.9       4.990   \n",
       "\n",
       "      mean_d_1_a  mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  \\\n",
       "0          0.411     -15.700       2.060        3.15  ...      23.50   \n",
       "1          1.680       2.880       3.830       -4.82  ...     -23.30   \n",
       "2          3.360      90.200      89.900        2.03  ...     462.00   \n",
       "3         -0.284       8.820       2.300       -1.97  ...     299.00   \n",
       "4         -5.790       3.060      41.400        5.52  ...      12.00   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2127      -2.030       0.647      -0.121       -1.10  ...     -21.70   \n",
       "2128       1.090       4.460       4.720        6.63  ...     594.00   \n",
       "2129       6.900      12.700       2.030        4.64  ...     370.00   \n",
       "2130       1.580     -16.000       1.690        4.74  ...     124.00   \n",
       "2131       1.950       6.210       3.490       -3.51  ...       1.95   \n",
       "\n",
       "      fft_742_b  fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  \\\n",
       "0        20.300     20.300      23.50     -215.0     280.00    -162.00   \n",
       "1       -21.800    -21.800     -23.30      182.0       2.57     -31.60   \n",
       "2      -233.000   -233.000     462.00     -267.0     281.00    -148.00   \n",
       "3      -243.000   -243.000     299.00      132.0     -12.40       9.53   \n",
       "4        38.100     38.100      12.00      119.0     -17.60      23.90   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2127      0.218      0.218     -21.70       95.2     -19.90      47.20   \n",
       "2128   -324.000   -324.000     594.00      -35.5     142.00     -59.80   \n",
       "2129   -160.000   -160.000     370.00      408.0    -169.00     -10.50   \n",
       "2130    -27.600    -27.600     124.00     -656.0     552.00    -271.00   \n",
       "2131      1.810      1.810       1.95      110.0      -6.71      22.80   \n",
       "\n",
       "      fft_748_b  fft_749_b  label  \n",
       "0       -162.00     280.00      1  \n",
       "1        -31.60       2.57     -1  \n",
       "2       -148.00     281.00      0  \n",
       "3          9.53     -12.40      0  \n",
       "4         23.90     -17.60     -1  \n",
       "...         ...        ...    ...  \n",
       "2127      47.20     -19.90     -1  \n",
       "2128     -59.80     142.00      0  \n",
       "2129     -10.50    -169.00      1  \n",
       "2130    -271.00     552.00      1  \n",
       "2131      22.80      -6.71     -1  \n",
       "\n",
       "[2132 rows x 2549 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the label of the class from Negative, Neutral and Positive to -1,0 and 1 respectively\n",
    "change = ({'NEUTRAL': -1, 'POSITIVE': 0, 'NEGATIVE': 1} )\n",
    "\n",
    "emotion1 = emotion.replace(change)\n",
    "\n",
    "emotion1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#change data to numpy array\n",
    "a = emotion1.to_numpy()\n",
    "\n",
    "X = a[:,:2548]\n",
    "y = a[:,2548]\n",
    "# split data training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.34 Stdev: 0.003 [SVC]\n",
      "Accuracy: 0.96 Stdev: 0.017 [Decision tree]\n",
      "Accuracy: 0.89 Stdev: 0.028 [KNN]\n",
      "Accuracy: 0.98 Stdev: 0.013 [Random Forest]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0))\n",
    "\n",
    "pipe2 = make_pipeline(DecisionTreeClassifier(max_depth=6,\n",
    "                                             criterion='entropy',\n",
    "                                             random_state=0))\n",
    "\n",
    "pipe3 = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=20,\n",
    "                                                             p=3,\n",
    "                                                             metric='minkowski'))\n",
    "pipe4 = make_pipeline(RandomForestClassifier(n_estimators=100, random_state = 0))\n",
    "\n",
    "\n",
    "clf_labels = ['SVC', 'Decision tree', 'KNN', 'Random Forest']\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, pipe3, pipe4], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34 Stdev: 0.003 [SVC]\n",
      "Accuracy: 0.96 Stdev: 0.017 [Decision tree]\n",
      "Accuracy: 0.89 Stdev: 0.028 [KNN]\n",
      "Accuracy: 0.98 Stdev: 0.013 [Random Forest]\n",
      "Accuracy: 0.94 Stdev: 0.02 [Majority voting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "mv_clf = VotingClassifier(estimators=[('svc', pipe1), ('dt', pipe2), ('kn', pipe3), ('rf', pipe4)])\n",
    "\n",
    "clf_labels += ['Majority voting']\n",
    "all_clf = [pipe1, pipe2, pipe3, pipe4, mv_clf]\n",
    "\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 430\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.328125\n"
     ]
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe1.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe1.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 10\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe2.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 59\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.9078125\n"
     ]
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe3.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 7\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.9890625\n"
     ]
    }
   ],
   "source": [
    "pipe4.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe4.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 31\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.9515625\n"
     ]
    }
   ],
   "source": [
    "mv_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mv_clf.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', mv_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 204\n",
      "Out of a total of: 640\n",
      "Accuracy: 0.68125\n",
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.66 Stdev: 0.02 ['Multi Layer Precepteron']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(64,64,64),activation=\"logistic\" ,random_state=1, max_iter=2000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', clf.score(X_test, y_test))\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "         \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" ['Multi Layer Precepteron']\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Accuracy from test split | 10-fold cross validation accuracy|\n",
    "|-------|--------------------------|----------------------------------|\n",
    "|  SVC  | 0.33 | 0.34 |\n",
    "|Decision Tree| 0.98 | 0.96 |\n",
    "|  KNN  | 0.91 | 0.89 |\n",
    "| Random Forest | 0.99 | 0.98 |\n",
    "| Majority Voting (above 4) | 0.95 | 0.94 |\n",
    "| Multi Neuron Percepteron | 0.68 | 0.66 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
